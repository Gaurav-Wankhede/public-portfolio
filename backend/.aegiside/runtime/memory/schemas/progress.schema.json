{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "required": ["schema_version", "reinforcement_learning"],
  "properties": {
    "schema_version": {"type": "string", "pattern": "^\\d+\\.\\d+\\.\\d+$"},
    "last_updated": {"type": "string", "format": "date-time"},
    "metrics": {
      "type": "object",
      "description": "SOURCE OF TRUTH - activeContext.metrics syncs total_rl_score from here",
      "required": ["total_rl_score", "tasks_completed", "commits"],
      "properties": {
        "total_rl_score": {"type": "number"},
        "tasks_completed": {"type": "integer", "minimum": 0},
        "tasks_failed": {"type": "integer", "minimum": 0},
        "commits": {"type": "integer", "minimum": 0}
      }
    },
    "reinforcement_learning": {
      "type": "array",
      "description": "TOP-APPEND MANDATORY: Newest transaction MUST be at [0]. RL ledger with PPO+GAE architecture.",
      "items": {
        "type": "object",
        "required": ["tx_id", "timestamp", "category", "rl_computation"],
        "properties": {
          "tx_id": {"type": "string"},
          "timestamp": {"type": "string", "format": "date-time"},
          "category": {"type": "string"},
          "reward_base": {"type": "number", "description": "Base reward before KL adjustment"},
          "penalty": {"type": "number", "description": "Penalty amount (negative)"},
          "source_file": {"type": "string"},
          "description": {"type": "string"},
          "rl_computation": {
            "type": "object",
            "required": ["td_error", "gae_advantage", "kl_divergence"],
            "description": "MANDATORY: Actual PPO+GAE calculations. NO raw integers allowed.",
            "properties": {
              "td_error": {"type": "number", "description": "δ_t = r_t + γV(s_{t+1}) - V(s_t)"},
              "gae_advantage": {"type": "number", "description": "A^GAE = Σ(γλ)^k × δ_{t+k} where γ=0.99, λ=0.95"},
              "kl_divergence": {"type": "number", "description": "KL(π_new || π_ref) = Σπ_new×log(π_new/π_ref)"},
              "value_updated": {"type": "number", "description": "V(s) + α × TD_error"},
              "policy_probabilities": {"type": "object", "description": "π(a|s) for all actions"},
              "monte_carlo_return": {"type": "number", "description": "G_t = Σγ^k × r_{t+k}"},
              "kl_penalty": {"type": "number", "description": "-kl_coef × KL where kl_coef=0.005"},
              "final_score": {"type": "number", "description": "reward_base × exp(-0.005×KL) - penalty"}
            }
          }
        }
      }
    },
    "rl_architecture": {
      "type": "object",
      "description": "PPO+GAE RL system configuration",
      "properties": {
        "algorithm": {"type": "string", "default": "PPO"},
        "epsilon": {"type": "number", "default": 0.2, "description": "PPO clip range for policy updates"},
        "kl_coefficient": {"type": "number", "default": 0.005},
        "gae_params": {
          "type": "object",
          "properties": {
            "gamma": {"type": "number", "default": 0.99},
            "lambda": {"type": "number", "default": 0.95}
          }
        },
        "exploit_explore_ratio": {
          "type": "object",
          "properties": {
            "exploitation": {"type": "number", "default": 0.7},
            "exploration": {"type": "number", "default": 0.3}
          }
        },
        "value_network": {"type": "string", "default": "multi-branch"},
        "reward_design": {"type": "string", "default": "automated_llm"},
        "n_step_td_learning": {
          "type": "object",
          "description": "Temporal Difference n-step returns for credit assignment across multi-step tasks. Formula: V(s) ← V(s) + α[r + γV(s') - V(s)]",
          "properties": {
            "n_steps": {"type": "integer", "default": 3, "description": "Lookahead horizon for TD(n)"},
            "discount_gamma": {"type": "number", "default": 0.99, "description": "Discount factor for future rewards"},
            "td_error_threshold": {"type": "number", "default": 0.1, "description": "Trigger learning update when |TD_error| > threshold"},
            "sub_goal_rewards": {
              "type": "array",
              "description": "Track intermediate sub-goal rewards for n-step returns",
              "items": {
                "type": "object",
                "properties": {
                  "step": {"type": "integer"},
                  "gae_advantage": {"type": "number", "description": "GAE advantage: Σ(γλ)^k × δ_{t+k}"},
                  "kl_divergence": {"type": "number", "description": "KL(π_new || π_ref) for policy stability"},
                  "rl_computation": {
                    "type": "object",
                    "description": "Actual RL formula calculations for this transaction",
                    "properties": {
                      "td_error": {"type": "number", "description": "r_t + γV(s_{t+1}) - V(s_t)"},
                      "value_updated": {"type": "number", "description": "V(s) + α*TD_error"},
                      "policy_probabilities": {"type": "object", "description": "Softmax π(a) = exp(Q/τ)/Σexp(Q/τ)"},
                      "monte_carlo_return": {"type": "number", "description": "G_t = Σγ^k*r_{t+k}"}
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "current_focus": {
      "type": "object",
      "properties": {
        "task": {"type": "string"},
        "tests_ready": {"type": "array", "items": {"type": "string"}}
      }
    },
    "resource_usage": {
      "type": "object",
      "description": "UX Gap 6 fix: Track API calls and costs to prevent budget surprises",
      "properties": {
        "api_calls": {
          "type": "object",
          "properties": {
            "context7": {"type": "integer", "minimum": 0},
            "exa": {"type": "integer", "minimum": 0},
            "fetch": {"type": "integer", "minimum": 0}
          }
        },
        "estimated_cost_usd": {"type": "number", "minimum": 0, "description": "Running total cost estimate"},
        "budget_limit_usd": {"type": "number", "default": 10.0, "description": "User-defined budget limit"},
        "alert_threshold": {"type": "number", "default": 0.8, "description": "Alert when cost reaches 80% of budget"},
        "budget_exceeded": {"type": "boolean", "default": false},
        "last_alert": {"type": "string", "format": "date-time"}
      }
    },
    "autonomy_settings": {
      "type": "object",
      "description": "UX Gap 10 fix: User-adjustable autonomy level for different work contexts",
      "properties": {
        "level": {"type": "integer", "minimum": 0, "maximum": 100, "default": 99, "description": "0=ask everything, 99=autonomous"},
        "temporary_override": {"type": "integer", "minimum": 0, "maximum": 100, "description": "Temporary autonomy level (e.g., 50 for production deploy)"},
        "override_reason": {"type": "string", "description": "Why autonomy was reduced"},
        "expires_at": {"type": "string", "format": "date-time", "description": "When to restore original autonomy"},
        "last_changed": {"type": "string", "format": "date-time"}
      }
    },
    "notification_preferences": {
      "type": "object",
      "description": "UX Gap 16 fix: User control over notification verbosity",
      "properties": {
        "verbosity": {"type": "string", "enum": ["silent", "minimal", "normal", "verbose"], "default": "normal"},
        "notify_on": {
          "type": "object",
          "properties": {
            "task_start": {"type": "boolean", "default": true},
            "task_complete": {"type": "boolean", "default": true},
            "error": {"type": "boolean", "default": true},
            "high_risk_action": {"type": "boolean", "default": true},
            "budget_warning": {"type": "boolean", "default": true},
            "validation_fail": {"type": "boolean", "default": true},
            "progress_updates": {"type": "boolean", "default": true},
            "rl_score_changes": {"type": "boolean", "default": false}
          }
        }
      }
    },
    "rl_model_state": {
      "type": "object",
      "description": "Gap 17 fix: RL model persistence and rollback capability",
      "properties": {
        "model_version": {"type": "string", "description": "e.g., v1.2.3"},
        "last_checkpoint": {"type": "string", "description": "Git commit SHA of last model snapshot"},
        "checkpoint_timestamp": {"type": "string", "format": "date-time"},
        "policy_snapshot_ref": {"type": "string", "description": "Path to serialized policy state"},
        "value_network_snapshot_ref": {"type": "string", "description": "Path to value network weights"},
        "performance_at_checkpoint": {"type": "number", "description": "Total RL score at snapshot"}
      }
    },
    "rl_runtime_controls": {
      "type": "object",
      "description": "Gap 18 fix: User-adjustable exploration/exploitation balance",
      "properties": {
        "exploit_ratio": {"type": "number", "minimum": 0, "maximum": 1, "default": 0.70, "description": "Reuse proven patterns"},
        "explore_ratio": {"type": "number", "minimum": 0, "maximum": 1, "default": 0.30, "description": "Try new approaches"},
        "epsilon_greedy": {"type": "number", "minimum": 0, "maximum": 1, "default": 0.1, "description": "Random exploration probability"},
        "epsilon_decay": {"type": "number", "default": 0.995, "description": "Decay rate per task"},
        "user_adjustable": {"type": "boolean", "default": true},
        "last_adjusted": {"type": "string", "format": "date-time"}
      }
    },
    "reward_breakdown": {
      "type": "object",
      "description": "Gap 19 fix: Transparent reward function calculation with ICLR 2025 process rewards",
      "properties": {
        "process_reward_categories": {
          "type": "object",
          "description": "Process rewards per ICLR 2025 moral alignment research",
          "properties": {
            "step_completion": {
              "type": "object",
              "properties": {
                "count": {"type": "integer", "minimum": 0},
                "total_reward": {"type": "number", "minimum": 0},
                "average_reward": {"type": "number", "minimum": 0},
                "base_range": {"type": "array", "items": {"type": "number"}, "minItems": 2, "maxItems": 2}
              }
            },
            "insight_generation": {
              "type": "object",
              "properties": {
                "count": {"type": "integer", "minimum": 0},
                "total_reward": {"type": "number", "minimum": 0},
                "average_reward": {"type": "number", "minimum": 0},
                "base_range": {"type": "array", "items": {"type": "number"}, "minItems": 2, "maxItems": 2}
              }
            },
            "milestone_achievement": {
              "type": "object",
              "properties": {
                "count": {"type": "integer", "minimum": 0},
                "total_reward": {"type": "number", "minimum": 0},
                "average_reward": {"type": "number", "minimum": 0},
                "base_range": {"type": "array", "items": {"type": "number"}, "minItems": 2, "maxItems": 2}
              }
            },
            "task_completion": {
              "type": "object",
              "properties": {
                "count": {"type": "integer", "minimum": 0},
                "total_reward": {"type": "number", "minimum": 0},
                "average_reward": {"type": "number", "minimum": 0},
                "base_range": {"type": "array", "items": {"type": "number"}, "minItems": 2, "maxItems": 2}
              }
            },
            "moral_alignment": {
              "type": "object",
              "properties": {
                "count": {"type": "integer", "minimum": 0},
                "total_reward": {"type": "number", "minimum": 0},
                "average_reward": {"type": "number", "minimum": 0},
                "base_range": {"type": "array", "items": {"type": "number"}, "minItems": 2, "maxItems": 2}
              }
            }
          }
        },
        "trajectory_analysis": {
          "type": "object",
          "description": "MONA-inspired trajectory analysis for reward hacking detection",
          "properties": {
            "reward_sequence": {
              "type": "array",
              "items": {"type": "number"},
              "description": "Sequence of rewards received"
            },
            "action_sequence": {
              "type": "array",
              "items": {"type": "string"},
              "description": "Sequence of actions taken"
            },
            "anomaly_score": {
              "type": "number",
              "minimum": 0,
              "maximum": 1,
              "description": "MONA anomaly detection score"
            },
            "reward_hacking_detected": {
              "type": "boolean",
              "description": "Whether reward hacking patterns were detected"
            },
            "reward_sequence_patterns": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "pattern": {"type": "string"},
                  "frequency": {"type": "number", "minimum": 0},
                  "risk_level": {"type": "string", "enum": ["low", "medium", "high", "critical"]}
                }
              }
            }
          }
        },
        "monitorability_tax": {
          "type": "object",
          "description": "Transparency and reasoning monitoring tax per OpenAI 2025",
          "properties": {
            "enabled": {"type": "boolean"},
            "current_tax_rate": {"type": "number", "minimum": 0, "maximum": 0.5},
            "total_tax_applied": {"type": "number"},
            "tax_triggers": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "trigger": {"type": "string"},
                  "tax_amount": {"type": "number"},
                  "timestamp": {"type": "string", "format": "date-time"}
                }
              }
            },
            "obfuscation_detection_score": {
              "type": "number",
              "minimum": 0,
              "maximum": 1,
              "description": "OpenAI o3 obfuscation monitoring score"
            }
          }
        },
        "last_task_rewards": {
          "type": "object",
          "properties": {
            "base_score": {"type": "number", "description": "Task completion base"},
            "quality_bonus": {"type": "number", "description": "Validation pass, EMD compliance"},
            "speed_bonus": {"type": "number", "description": "Efficiency reward"},
            "user_rating_bonus": {"type": "number", "description": "Explicit user feedback"},
            "pattern_reuse_bonus": {"type": "number", "description": "+20 per reuse"},
            "mcp_integration_bonus": {"type": "number", "description": "+10 per complete chain"},
            "total": {"type": "number", "description": "Sum of all components"}
          }
        },
        "calculation_formula": {"type": "string", "default": "total = base + quality + speed + user_rating + pattern_reuse + mcp - monitorability_tax"}
      }
    },
    "value_network_branches": {
      "type": "object",
      "description": "Gap 20 fix: Multi-branch value network weighting details",
      "properties": {
        "branch_weights": {
          "type": "object",
          "properties": {
            "task_success": {"type": "number", "default": 0.30, "description": "Did task complete?"},
            "code_quality": {"type": "number", "default": 0.25, "description": "Validation, EMD, security"},
            "user_satisfaction": {"type": "number", "default": 0.20, "description": "User rating, feedback"},
            "efficiency": {"type": "number", "default": 0.15, "description": "Time, resource usage"},
            "learning": {"type": "number", "default": 0.10, "description": "Pattern creation, knowledge graph"}
          }
        },
        "weight_sum_constraint": {"type": "number", "const": 1.0, "description": "Weights must sum to 1.0"},
        "auto_adjust": {"type": "boolean", "default": false, "description": "LLM adjusts based on success patterns"},
        "last_adjustment": {"type": "string", "format": "date-time"}
      }
    }
  }
}
